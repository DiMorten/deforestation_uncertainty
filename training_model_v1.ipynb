{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbd5b7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_v1 import *\n",
    "root_path = 'F:/Doctorado/ForstCARe/Images_Para_10m/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64080745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask label shape:  \n",
      " (17730, 9203) \n",
      " Unique values:  \n",
      " [0. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "# Loading reference\n",
    "label_mask = np.load(root_path + 'References/mask_label_17730x9203.npy').astype('float32')\n",
    "print('Mask label shape: ', '\\n', label_mask.shape, '\\n', 'Unique values: ', '\\n', np.unique(label_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4819ba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiles size:  3546 2300\n",
      "Mask size:  (17730, 9200)\n"
     ]
    }
   ],
   "source": [
    "# Creating tile mask\n",
    "grid_x, grid_y = 5,4\n",
    "mask_tiles = create_mask(label_mask.shape[0], label_mask.shape[1], grid_size=(grid_x, grid_y))\n",
    "label_mask = label_mask[:mask_tiles.shape[0], :mask_tiles.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31bb7ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  (17730, 9203, 20)\n",
      "mask:  (17730, 9200)\n",
      "image stack:  (17730, 9200, 20)\n",
      "ref : (17730, 9200)\n"
     ]
    }
   ],
   "source": [
    "# Loading image stack\n",
    "image_stack = np.load('F:/Doctorado/ForstCARe/Code_ResUNet/Sets_S2/raw/img_filt_norm_2018_2019_raw.npy').astype('float32')\n",
    "print('Image shape: ', image_stack.shape)\n",
    "channels = image_stack.shape[-1]\n",
    "image_stack = image_stack[:mask_tiles.shape[0], :mask_tiles.shape[1],:]\n",
    "print('mask: ',mask_tiles.shape)\n",
    "print('image stack: ', image_stack.shape)\n",
    "print('ref :', label_mask.shape)\n",
    "#plt.imshow(mask_tiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6c5f2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tiles:  [1, 3, 5, 8, 11, 13, 14, 20]\n",
      "Validation tiles:  [6, 19]\n",
      "Test tiles:  [2, 4, 7, 9, 10, 12, 15, 16, 17, 18]\n"
     ]
    }
   ],
   "source": [
    "# Defining tiles for training, validation and test sets\n",
    "tiles_tr = [1,3,5,8,11,13,14,20] \n",
    "tiles_val = [6,19]\n",
    "tiles_ts = list(set(np.arange(grid_x * grid_y)+1)-set(tiles_tr)-set(tiles_val))\n",
    "    \n",
    "print('Training tiles: ', tiles_tr)\n",
    "print('Validation tiles: ', tiles_val)\n",
    "print('Test tiles: ', tiles_ts)\n",
    "\n",
    "# Training and validation mask\n",
    "mask_tr_val = np.zeros((mask_tiles.shape)).astype('float32')\n",
    "\n",
    "for tr_ in tiles_tr:\n",
    "    mask_tr_val[mask_tiles == tr_] = 1\n",
    "\n",
    "for val_ in tiles_val:\n",
    "    mask_tr_val[mask_tiles == val_] = 2\n",
    "\n",
    "mask_amazon_ts = np.zeros((mask_tiles.shape)).astype('float32')\n",
    "for ts_ in tiles_ts:\n",
    "    mask_amazon_ts[mask_tiles == ts_] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45271c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting patches from the idx matrix\n",
    "overlap = 0.7\n",
    "patch_size = 128\n",
    "batch_size = 32\n",
    "im_idx = create_idx_image(label_mask)\n",
    "patches_idx = extract_patches(im_idx, patch_size=(patch_size, patch_size), overlap=overlap).reshape(-1,patch_size, patch_size)\n",
    "patches_mask = extract_patches(mask_tr_val, patch_size=(patch_size, patch_size), overlap=overlap).reshape(-1, patch_size, patch_size)\n",
    "del im_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f83b95b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training and validation patches:   41812 10260\n"
     ]
    }
   ],
   "source": [
    "# Selecting index trn val and test patches idx\n",
    "idx_trn = np.squeeze(np.where(patches_mask.sum(axis=(1, 2))==patch_size**2))\n",
    "idx_val = np.squeeze(np.where(patches_mask.sum(axis=(1, 2))==2*patch_size**2))\n",
    "del patches_mask\n",
    "\n",
    "patches_idx_trn = patches_idx[idx_trn]\n",
    "patches_idx_val = patches_idx[idx_val]\n",
    "del idx_trn, idx_val\n",
    "\n",
    "print('Number of training and validation patches:  ', len(patches_idx_trn), len(patches_idx_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76199f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training samples:  (3753, 128, 128) validation samples:  (1033, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "# Keeping patches with 2% of def class\n",
    "X_train = retrieve_idx_percentage(label_mask, patches_idx_trn, patch_size, pertentage = 0.2)\n",
    "X_valid = retrieve_idx_percentage(label_mask, patches_idx_val, patch_size, pertentage = 0.2)\n",
    "print('training samples: ', X_train.shape, 'validation samples: ', X_valid.shape)\n",
    "del patches_idx_trn, patches_idx_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9808000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train and validation data generator\n",
    "train_datagen = ImageDataGenerator()\n",
    "valid_datagen = ImageDataGenerator()\n",
    "\n",
    "y_train = np.zeros((len(X_train)))\n",
    "y_valid = np.zeros((len(X_valid)))\n",
    "\n",
    "len_X_train = len(X_train)\n",
    "len_X_valid = len(X_valid)\n",
    "\n",
    "train_gen = train_datagen.flow(np.expand_dims(X_train, axis = -1), y_train,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True)\n",
    "\n",
    "valid_gen = valid_datagen.flow(np.expand_dims(X_valid, axis = -1), y_valid,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False)\n",
    "\n",
    "del X_train, X_valid\n",
    "number_class = 3\n",
    "train_gen_batch = batch_generator(train_gen, image_stack, label_mask, patch_size, number_class)\n",
    "valid_gen_batch = batch_generator(valid_gen, image_stack, label_mask, patch_size, number_class)\n",
    "#del image_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034a74fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating folder for the experiment\n",
    "exp = 0\n",
    "path_exp = 'F:/Doctorado/ForstCARe/Code_ResUNet/Experiments/exp'+str(exp)\n",
    "path_models = path_exp+'/models'\n",
    "path_maps = path_exp+'/pred_maps'\n",
    "\n",
    "if not os.path.exists(path_exp):\n",
    "    os.makedirs(path_exp)   \n",
    "if not os.path.exists(path_models):\n",
    "    os.makedirs(path_models)   \n",
    "if not os.path.exists(path_maps):\n",
    "    os.makedirs(path_maps)\n",
    "    \n",
    "times = 3\n",
    "method = 'resunet'\n",
    "nb_filters = [16, 32, 64, 128, 256]\n",
    "weights = [0.1, 0.9, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6630ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all = []\n",
    "\n",
    "for tm in range(0,times):\n",
    "    print('time: ', tm)\n",
    "\n",
    "    rows = patch_size\n",
    "    cols = patch_size\n",
    "    adam = Adam(lr = 1e-3 , beta_1=0.9)\n",
    "    \n",
    "    loss = weighted_categorical_crossentropy(weights)\n",
    "    input_shape = (rows, cols, channels)\n",
    "    model = build_resunet(input_shape, nb_filters, number_class)\n",
    "    \n",
    "    model.compile(optimizer=adam, loss=loss, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    earlystop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=10, verbose=1, mode='min')\n",
    "    checkpoint = ModelCheckpoint(path_models+ '/' + method +'_'+str(tm)+'.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    lr_reduce = ReduceLROnPlateau(factor=0.9, min_delta=0.0001, patience=5, verbose=1)\n",
    "    callbacks_list = [earlystop, checkpoint]\n",
    "    # train the model\n",
    "    start_training = time.time()\n",
    "    history = model.fit_generator(train_gen_batch,\n",
    "                              steps_per_epoch=len_X_train*3//train_gen.batch_size,\n",
    "                              validation_data=valid_gen_batch,\n",
    "                              validation_steps=len_X_valid*3//valid_gen.batch_size,\n",
    "                              epochs=100,\n",
    "                              callbacks=callbacks_list)\n",
    "    end_training = time.time() - start_training\n",
    "    metrics_all.append(end_training)\n",
    "    del model, history\n",
    "\n",
    "# Saving training time\n",
    "np.save(path_exp+'/metrics_tr.npy', metrics_all)\n",
    "del train_gen_batch, valid_gen_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e741522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Test loop\n",
    "metrics_ts = []\n",
    "n_pool = 3\n",
    "n_rows = 5\n",
    "n_cols = 4\n",
    "rows, cols = image_stack.shape[:2]\n",
    "pad_rows = rows - np.ceil(rows/(n_rows*2**n_pool))*n_rows*2**n_pool\n",
    "pad_cols = cols - np.ceil(cols/(n_cols*2**n_pool))*n_cols*2**n_pool\n",
    "print(pad_rows, pad_cols)\n",
    "\n",
    "npad = ((0, int(abs(pad_rows))), (0, int(abs(pad_cols))), (0, 0))\n",
    "image1_pad = np.pad(image_stack, pad_width=npad, mode='reflect')\n",
    "del image_stack\n",
    "\n",
    "h, w, c = image1_pad.shape\n",
    "patch_size_rows = h//n_rows\n",
    "patch_size_cols = w//n_cols\n",
    "num_patches_x = int(h/patch_size_rows)\n",
    "num_patches_y = int(w/patch_size_cols)\n",
    "\n",
    "new_model = build_resunet(input_shape=(patch_size_rows,patch_size_cols, c), nb_filters = nb_filters, n_classes=3)\n",
    "\n",
    "metrics_all =[]\n",
    "\n",
    "for tm in range(0,times):\n",
    "    print('time: ', tm)\n",
    "    model = load_model(path_models+ '/' + method +'_'+str(tm)+'.h5', compile=False)\n",
    "    \n",
    "    for l in range(1, len(model.layers)):\n",
    "        new_model.layers[l].set_weights(model.layers[l].get_weights())\n",
    "    \n",
    "    start_test = time.time()\n",
    "    patch_t = []\n",
    "    \n",
    "    for i in range(0,num_patches_y):\n",
    "        for j in range(0,num_patches_x):\n",
    "            patch = image1_pad[patch_size_rows*j:patch_size_rows*(j+1), patch_size_cols*i:patch_size_cols*(i+1), :]\n",
    "            predictions_ = new_model.predict(np.expand_dims(patch, axis=0))\n",
    "            del patch \n",
    "            patch_t.append(predictions_[:,:,:,1])\n",
    "            del predictions_\n",
    "    ts_time =  time.time() - start_test\n",
    "    patches_pred = np.asarray(patch_t).astype(np.float32)\n",
    "    # Recinstructing predicted map\n",
    "    prob_recontructed = pred_reconctruct(h, w, num_patches_x, num_patches_y, patch_size_rows, patch_size_cols, patches_pred)\n",
    "    np.save(path_maps+'/'+'prob_'+str(tm)+'.npy',prob_recontructed) \n",
    "\n",
    "    metrics_all.append(ts_time)\n",
    "    del prob_recontructed, model, patches_pred\n",
    "metrics_ = np.asarray(metrics_all)\n",
    "del image1_pad\n",
    "# Saving test time\n",
    "np.save(path_exp+'/metrics_ts.npy', metrics_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc025ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_rec = np.zeros((h, w, times))\n",
    "\n",
    "for tm in range (0, times):\n",
    "    print(tm)\n",
    "    prob_rec[:,:,tm] = np.load(path_maps+'/'+'prob_'+str(tm)+'.npy').astype(np.float32)\n",
    "\n",
    "mean_prob = np.mean(prob_rec, axis = -1)\n",
    "np.save(path_maps+'/prob_mean.npy', mean_prob)\n",
    "\n",
    "fig1 = plt.figure(figsize=(10,10))\n",
    "plt.imshow(mean_prob, cmap = 'jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1316e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing metrics over the test tiles\n",
    "mean_prob = mean_prob[:label_mask.shape[0], :label_mask.shape[1]]\n",
    "ref1 = np.ones_like(label_mask).astype(np.float32)\n",
    "\n",
    "ref1 [label_mask == 2] = 0\n",
    "TileMask = mask_amazon_ts * ref1\n",
    "GTTruePositives = label_mask==1\n",
    "\n",
    "# Metrics for th=0.5    \n",
    "ProbList_05 = [0.5]\n",
    "\n",
    "metrics_05 = matrics_AA_recall(ProbList_05, mean_prob, label_mask, mask_amazon_ts, 625)\n",
    "print('Metrics th = 0.5: ', metrics_05*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0791592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
